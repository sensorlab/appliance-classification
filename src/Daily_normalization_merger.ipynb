{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd689fd-552e-4f2e-afc2-1341d1727549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make all possible combinations of provided datasets AKA disk destroyer 9000\n",
    "def merger(datasets,min_comb, max_comb):\n",
    "    import itertools\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    # Generate all combinations of the datasets\n",
    "    for i in range(min_comb, max_comb):\n",
    "        for comb in itertools.combinations(datasets, i):\n",
    "            merged = {}\n",
    "            # Merge the datasets\n",
    "            for dataset in comb:\n",
    "                with open(f'processed_data_final/{dataset}', 'rb') as handle:\n",
    "                    data = pickle.load(handle)\n",
    "                    for key in data:\n",
    "                        if key in merged:\n",
    "                            merged[key].extend(data[key])\n",
    "                        else:\n",
    "                            merged[key] = data[key]\n",
    "            # Extract the first three letters of each dataset name and join them\n",
    "            file_name = '_'.join([os.path.splitext(name)[0][:3] for name in comb]) + '.pkl'\n",
    "            # Save the merged dictionary as a pickled file with the first three letters of each dataset name in the file name\n",
    "            with open(f'testing_sets/{i}/{file_name}', 'wb') as handle:\n",
    "                pickle.dump(merged, handle)\n",
    "datasets = [\n",
    "        'UKDALE_processed.pkl', \n",
    "        'REFIT_processed.pkl', \n",
    "        'ECO_processed.pkl', \n",
    "        'SYND_processed.pkl', \n",
    "        'LERTA_processed.pkl', \n",
    "        'HES_processed.pkl', \n",
    "        'SMART_processed.pkl'\n",
    "    ]\n",
    "merger(datasets,6,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b14287-8649-499f-abb5-3143163e26d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take excess data out of the desired dictionary to save space\n",
    "import pickle\n",
    "\n",
    "path = 'SYND_processed.pkl'\n",
    "\n",
    "with open('processed_data/' + path, 'rb') as handle:\n",
    "    dataload = pickle.load(handle)\n",
    "\n",
    "# List of accepted device names\n",
    "accepted = ['microwave', 'oven', 'fridge', 'washing machine', 'dishwasher', 'kettle', 'television', 'dish washer', 'electric oven']\n",
    "\n",
    "# Filter data to only include accepted device names\n",
    "data = dataload.copy()\n",
    "\n",
    "for i, j in dataload.items():\n",
    "    if i not in accepted:\n",
    "        del data[i]\n",
    "\n",
    "print(\"//////////////////\")\n",
    "\n",
    "if 'dish washer' in data.keys():\n",
    "    data['dishwasher'] = data.pop('dish washer')\n",
    "\n",
    "if 'electric oven' in data.keys():\n",
    "    data['oven'] = data.pop('electric oven')\n",
    "\n",
    "\n",
    "with open('processed_data_final/' + path, 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5145219-e004-41b9-872e-5b2dd8b1c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_daily_slices(data):\n",
    "\n",
    "    # Extracts daily slices from the given data for a specific device and iteration.\n",
    "\n",
    "    daily_slices = []\n",
    "    current_day = None\n",
    "    current_slice = []\n",
    "    for i, (date, power) in enumerate(data[('power', 'active')].items()):\n",
    "        day_of_year = date.day_of_year\n",
    "        if day_of_year != current_day:\n",
    "            if current_day is not None:\n",
    "                daily_slices.append(data.iloc[current_slice])\n",
    "            current_day = day_of_year\n",
    "            current_slice = [i]\n",
    "        else:\n",
    "            current_slice.append(i)\n",
    "    if current_slice:\n",
    "        daily_slices.append(data.iloc[current_slice])\n",
    "    return daily_slices\n",
    "\n",
    "\n",
    "def daily_1(spr, length):\n",
    "    \n",
    "    # Extracts daily slices for each device in the given data and truncates the resulting slices to the given length.\n",
    "\n",
    "    result = {}\n",
    "    for device, data in tqdm(spr.items()):\n",
    "        daily_slices = []\n",
    "        for iteration in data:\n",
    "            daily_slices.extend(extract_daily_slices(iteration))\n",
    "        result[device] = daily_slices\n",
    "    result = truncate_data(result, length)\n",
    "    return result\n",
    "\n",
    "\n",
    "def truncate_data(data, length):\n",
    "\n",
    "    # Truncates the given data to the given length. Optional, for oversampling reasons\n",
    "\n",
    "    truncated_data = {}\n",
    "    for device, slices in data.items():\n",
    "        truncated_data[device] = slices[:length]\n",
    "    return truncated_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
