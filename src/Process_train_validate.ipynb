{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c0170f-ea07-4497-9556-6ea42f7a2ddc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports/Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad7c86f-8e41-40e4-bea0-386091114db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!mamba install tensorflow-gpu==2.10 -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80af8af7-8d4b-41a3-b754-3a16014191f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-1.7.3-py3-none-manylinux2014_x86_64.whl (193.6 MB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.9.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 13:24:52.539796: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA A100 80GB PCIe MIG 3g.40gb, compute capability 8.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from joblib import Memory\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "!pip install -q scikit-learn-intelex\n",
    "#from sklearnex import patch_sklearn\n",
    "#patch_sklearn()\n",
    "\n",
    "from sklearn import pipeline, metrics, linear_model, model_selection, multioutput, tree, ensemble, neural_network\n",
    "!pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from sklearn.utils import class_weight\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "gpus = tf.config.list_physical_devices('GPU') or []\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f421e43f-2e40-42e7-aa08-752bb0ec6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(location='./cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "524fa17d-4b41-481a-94a0-6de0b5642e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Tuple\n",
    "global display_graph_counter\n",
    "display_graph_counter=0\n",
    "\n",
    "\n",
    "\n",
    "def appliance_augmentator(dataset:Dict[str,list], sample_length:int, n_appliances_per_sample:int, random_state:int=None) -> Iterator[Tuple[np.ndarray, np.ndarray, List[str]]]:\n",
    "    global display_graph_counter\n",
    "   \n",
    "          \n",
    "    # Initialize seeded random number generator\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    \n",
    "    # How many appliances are mixed together\n",
    "    N = n_appliances_per_sample\n",
    "    \n",
    "    # Sample length / Window size\n",
    "    L = sample_length\n",
    "    \n",
    "    # Noise floor\n",
    "    NOISE_FLOOR = 20.0\n",
    "    \n",
    "    # Get all available appliances\n",
    "    appliance_names = tuple(dataset.keys())\n",
    "    \n",
    "    # How many appliances are there?\n",
    "    n_appliances = len(appliance_names)\n",
    "    #print(n_appliances)\n",
    "    #print(N)\n",
    "    # Start endless generator of samples\n",
    "    # pre-allocate array for time series\n",
    "    series = np.zeros(L, dtype=np.float64)\n",
    "        \n",
    "    # pre-allocate boolean array for masks\n",
    "    labels = np.zeros((n_appliances, L), dtype=bool)\n",
    "\n",
    "    if testing:\n",
    "        print(\"test run\")\n",
    "        appliance_names=naprave\n",
    "\n",
    "    print(appliance_names, \" pri augmentarorju\")\n",
    "    \n",
    "\n",
    "    # Select N random appliances (no replace, because same appliance should not appear twice in the same sample)\n",
    "    # TODO: Relax this limitation in the future, for cases where multiple of same appliances are in the same household\n",
    "    while True:\n",
    "        for _ in tqdm(range(120000)):\n",
    "            #print(_)\n",
    "            series = np.zeros(L, dtype=np.float64)\n",
    "\n",
    "            # pre-allocate boolean array for masks\n",
    "            labels = np.zeros((n_appliances, L), dtype=bool)\n",
    "\n",
    "            #print(\"///////////////////////////\")\n",
    "            for appliance_idx in rng.choice(n_appliances, size=N, replace=False):\n",
    "                #print(appliance_names[appliance_idx])\n",
    "                appliance_name = appliance_names[appliance_idx]\n",
    "                \n",
    "                # Pick random sample of a selected appliance\n",
    "                n_available_samples = len(dataset[appliance_name])\n",
    "                empty=True\n",
    "                while empty:\n",
    "                    n_available_samples = len(dataset[appliance_name])\n",
    "                    sample_idx = rng.choice(n_available_samples)\n",
    "                    #print(\"test1\")\n",
    "                    # retrieve sample as NumPy array with appropriate dimensions\n",
    " \n",
    "                    sample_series = dataset[appliance_name][sample_idx].iloc[:].to_numpy().squeeze(axis=-1)\n",
    "                        #with numpy.printoptions(threshold=numpy.inf):\n",
    "                            #print(sample_series)            \n",
    "                        # If sample is too short (shorter than L), give padding on both sides.\n",
    "                    if len(sample_series) <= L:\n",
    "                        padding = L // 2\n",
    "                        sample_series = np.pad(sample_series, (padding, padding), mode='constant', constant_values=0)\n",
    "                        #print(\"////////////////////////\")\n",
    "                        #with numpy.printoptions(threshold=numpy.inf):\n",
    "                            #print(sample_series)\n",
    "                        # The total length of sample time-series\n",
    "                    sample_len = len(sample_series)\n",
    "\n",
    "                        # Sanity check(s)\n",
    "                    assert sample_len >= L, f'Sample length should be equal or larger than L: {sample_len} >= {L}'\n",
    "\n",
    "                    sample_offset = rng.choice(sample_len - L)\n",
    "\n",
    "                    sample = sample_series[sample_offset:sample_offset+L]\n",
    "\n",
    "                        # TODO: Currently, we ignore device in idle state (x =< NOISE_FLOOR)\n",
    "                    mask = sample > NOISE_FLOOR  # find samples that are above noise floor\n",
    "                    if (np.any(mask)):\n",
    "                        if(appliance_name=='electric oven'):\n",
    "                            #print(sample_idx,\" :which sample\")\n",
    "                                \n",
    "                            #plt.plot(sample)\n",
    "                            #plt.title(appliance_name)\n",
    "                            #plt.show()\n",
    "                            pass\n",
    "                            \n",
    "                        series[:] += sample\n",
    "                        labels[appliance_idx, :] |= mask\n",
    "                        empty=False\n",
    "\n",
    "                    # logical ORing the mask\n",
    "\n",
    "                #print(np.sum([(np.any(label) > 0) for label in labels]), N)\n",
    "                # Add random (constant) offset\n",
    "                #series += rng.random() * (NOISE_FLOOR)\n",
    "\n",
    "                # There has to be two samples present. Even though we combined two subsets, one or both could be empty.\n",
    "                # Workaround until area of interest is implemented.\n",
    "            #print(np.shape(series))\n",
    "            #print(np.shape(labels))\n",
    "\n",
    "            display_graph_counter+=1\n",
    "            yield series, labels, appliance_names\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ab8174-52a7-4b41-831d-dfc7b2124fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progress\n",
      "  Using cached progress-1.6-py3-none-any.whl\n",
      "Installing collected packages: progress\n",
      "Successfully installed progress-1.6\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "!pip install progress\n",
    "from progress.bar import Bar\n",
    "\n",
    "def DevicesDataXY(number_of_datasets: int, \n",
    "                  number_of_devices_in_datasets: int, \n",
    "                  number_of_all_devices: int):\n",
    "    \n",
    "    # This function uses processed_data and shapes it into two lists \n",
    "    # one of them contains data from devices (dataX_Y) and one of them names of devices (devicesX_Y).\n",
    "    # Lists contain multiple datasets specified with number_of_datasets, all of which have \n",
    "    # the same number of devices in them, specified by number_of_devices_in_datasets.\n",
    "    # We choose devices for datasets randomly, thus we have to specify the number of all devices in the REFIT (22) or UKDALE dataset (54).\n",
    "    \n",
    "    # we extract the dictionary processed_data into a list AllTable\n",
    "    #print(processed_data.keys())\n",
    "    #'boiler', 'solar thermal pumping station', 'laptop computer', 'washer dryer', 'dish washer'])\n",
    "    #('laptop computer', 'solar thermal pumping station', 'boiler', 'dish washer', 'washer dryer')\n",
    "    AllTable = [[k,v] for k,v in processed_data.items()]\n",
    "    #print(AllTable[0][0])\n",
    "    devicesX_Y = []\n",
    "    dataX_Y = []\n",
    "        \n",
    "\n",
    "            \n",
    "    # for loop goes over all datasets\n",
    "    for i in range(0,number_of_datasets):    \n",
    "        devicesY = []\n",
    "        dataY = []\n",
    "        j = 0\n",
    "        \n",
    "        # while loop goes over all devices in datasets\n",
    "        while j < number_of_devices_in_datasets:\n",
    "            \n",
    "            # we get a random number with random library\n",
    "            random_number = random.randrange(number_of_all_devices)\n",
    "            \n",
    "            # we use the random number to get a random device and random data that belongs to it from AllTable\n",
    "            random_device = AllTable[random_number][0]\n",
    "            random_data = AllTable[random_number][1]\n",
    "            \n",
    "            # we append the device and its data if it doesn't already exist \n",
    "            # and thus avoid having the same device more then once in the same dataset\n",
    "            if random_device not in devicesY:\n",
    "                devicesY.append(random_device)\n",
    "                dataY.append(random_data)\n",
    "                j += 1\n",
    "        \n",
    "        devicesX_Y.append(devicesY)\n",
    "        dataX_Y.append(dataY)\n",
    "    \n",
    "    return devicesX_Y, dataX_Y\n",
    "\n",
    "\n",
    "def ListsToDictlist(devices: list, data: list, number_of_devices: int):\n",
    "    \n",
    "    # This function takes lists from DevicesDataXY or DevicesDataHoly5 and\n",
    "    # turns each of the sublists into a dictionary and then appends those dicitonaries into a list. \n",
    "    \n",
    "    Dictlist=[]\n",
    "    \n",
    "    # we make dictionaries out of lists that we input and append them to list of dictionaries (Dictlist)\n",
    "    for i in range(0,number_of_devices):\n",
    "        dictionary=dict(zip(devices[i], data[i]))\n",
    "        Dictlist.append(dictionary)\n",
    "        \n",
    "    return Dictlist\n",
    "\n",
    "def Generate4Upgraded(Dictlist: list, \n",
    "              sample_length: int, \n",
    "              n_appliances_per_sample: int, \n",
    "              dataset_number: int,\n",
    "              number_of_generated_samples: int\n",
    "                     ):\n",
    "    \n",
    "    # This function does the same as Generate4 except here we can specify the number of all generated samples. \n",
    "    \n",
    "    X, Y, labels = [], [], None\n",
    "    \n",
    "    generator = appliance_augmentator(Dictlist[dataset_number], \n",
    "                                      sample_length=sample_length, \n",
    "                                      n_appliances_per_sample=n_appliances_per_sample,\n",
    "                                      #n_samples=number_of_generated_samples,\n",
    "                                      random_state=0xDEADBEEF)\n",
    "    for idx, (_x, _y, labels) in zip((range(number_of_generated_samples)), generator):\n",
    "        X.append(_x), Y.append(_y)\n",
    "  \n",
    "    X, Y = np.asarray(X), np.asarray(Y)\n",
    "    y = np.any(Y, axis=-1) > 0\n",
    "    sample_weight = np.sum(Y, axis=-1) / 128\n",
    "    \n",
    "    # split into train test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # add the needed dimension\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, labels\n",
    "\n",
    "\n",
    "def class_weights_tool(y_test):\n",
    "    \n",
    "    # This function returns class weights for each device in a prticular dataset in a form of a dicitionary.\n",
    "    # It does so by simply counting how many times the device is present througout the dataset.\n",
    "    \n",
    "    # inspired by ronnie coleman\n",
    "    light_weight, nums = [], []\n",
    "    \n",
    "    # this for loop goes over the collumns of the y_test dataset\n",
    "    for j in range(0,len(y_test[0])):             \n",
    "        \n",
    "        count=0\n",
    "        \n",
    "        # gives 0,1,2,3,4,5,6....\n",
    "        nums.append(j)\n",
    "        \n",
    "        # this loop goes over the rows in the y_test dataset\n",
    "        for i in range(0,len(y_test)):\n",
    "            \n",
    "            # we count Trues in the whole column of y_test dataset\n",
    "            if y_test[i][j] == True: count+=1     \n",
    "        \n",
    "        # we append Trues for the column of y_test dataset to the list light_weight\n",
    "        light_weight.append(count)\n",
    "        \n",
    "    # makes the dictionary    \n",
    "    class_weights_dictionary = dict(zip(nums, light_weight))             \n",
    "    \n",
    "    return class_weights_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a16897-9c76-4ea1-b9e6-4bfc5eb92f34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b60a151-76af-415c-bf48-e647a90838ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from joblib import Memory\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input, GRU, BatchNormalization, LSTM, Bidirectional, AveragePooling1D\n",
    "from keras.layers import Conv1D, Conv1DTranspose, LocallyConnected1D, SeparableConv1D, ConvLSTM1D\n",
    "from keras.layers import MaxPooling1D, Dropout\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "#from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "#from keras.engine.topology import get_source_inputs\n",
    "\n",
    "def LOL2(classes,\n",
    "             window_size,\n",
    "             method,\n",
    "             method_num,\n",
    "             include_top=True,\n",
    "             input_tensor=None,\n",
    "              pooling=None):\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = (window_size,1)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    # Block 1\n",
    "    x = Conv1D(64, (3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv1D(64, (3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = AveragePooling1D((2), strides=(2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv1D(128, (3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv1D(128, (3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = AveragePooling1D((2), strides=(2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv1D(256, (3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv1D(256, (3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = AveragePooling1D((2), strides=(2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv1D(512, (3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv1D(512, (3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = AveragePooling1D((2), strides=(2), name='block4_pool')(x)\n",
    "    \n",
    "    # Block 5\n",
    "    x = Conv1DTranspose(512, (3), activation='relu', padding='same', name='block5_tran_conv1')(x)\n",
    "    #x = Conv1DTranspose(512, (3), activation='relu', padding='same', name='block5_tran_conv2')(x)\n",
    "    x = AveragePooling1D((2), strides=(2), name='block5_pool')(x)\n",
    "    \n",
    "    # GRU layer, btw GRU stands for Gated Recurrent Units; https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
    "    if method == 'gru':\n",
    "        print(\"GRU enabled\")\n",
    "        x = GRU(method_num, activation='tanh', recurrent_activation='sigmoid')(x)                \n",
    "        \n",
    "    if method == 'gru2':\n",
    "        print(\"GRU enabled\")\n",
    "        x = GRU(method_num,activation='tanh',recurrent_activation='sigmoid',reset_after=True)(x)        \n",
    "        \n",
    "    if method == 'bigru':\n",
    "        print(\"Bi-GRU enabled\")\n",
    "        x = Bidirectional(GRU(method_num, activation='tanh', recurrent_activation='sigmoid'))(x)\n",
    "        \n",
    "    if method == 'lstm':\n",
    "        print(\"LSTM enabled\")\n",
    "        x = LSTM(method_num, activation='tanh', recurrent_activation='sigmoid')(x)\n",
    "    \n",
    "    # LSTM layer, btw LSTM stands for long short-term memory; https://en.wikipedia.org/wiki/Long_short-term_memory\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = Flatten(name='flatten')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = Dense(classes, activation='sigmoid', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='LOL2')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9188bac0-4989-4bf0-af61-30da21fb0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate4Upgraded(Dictlist: list, \n",
    "              sample_length: int, \n",
    "              n_appliances_per_sample: int, \n",
    "              dataset_number: int,\n",
    "              number_of_generated_samples: int\n",
    "                     ):\n",
    "    \n",
    "    # This function does the same as Generate4 except here we can specify the number of all generated samples. \n",
    "    \n",
    "    X, Y, labels = [], [], None\n",
    "    \n",
    "    generator = appliance_augmentator(Dictlist[dataset_number], \n",
    "                                      sample_length=sample_length, \n",
    "                                      n_appliances_per_sample=n_appliances_per_sample,\n",
    "                                      #n_samples=number_of_generated_samples,\n",
    "                                      random_state=0xDEADBEEF)\n",
    "    for idx, (_x, _y, labels) in zip((range(number_of_generated_samples)), generator):\n",
    "        X.append(_x), Y.append(_y)\n",
    "  \n",
    "    X, Y = np.asarray(X), np.asarray(Y)\n",
    "    y = np.any(Y, axis=-1) > 0\n",
    "    sample_weight = np.sum(Y, axis=-1) / 128\n",
    "    \n",
    "    # split into train test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # add the needed dimension\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, labels\n",
    "\n",
    "\n",
    "def DevicesDataXY(number_of_datasets: int, \n",
    "                  number_of_devices_in_datasets: int, \n",
    "                  number_of_all_devices: int):\n",
    "    \n",
    "    # This function uses processed_data and shapes it into two lists \n",
    "    # one of them contains data from devices (dataX_Y) and one of them names of devices (devicesX_Y).\n",
    "    # Lists contain multiple datasets specified with number_of_datasets, all of which have \n",
    "    # the same number of devices in them, specified by number_of_devices_in_datasets.\n",
    "    # We choose devices for datasets randomly, thus we have to specify the number of all devices in the REFIT (22) or UKDALE dataset (54).\n",
    "    \n",
    "    # we extract the dictionary processed_data into a list AllTable\n",
    "    #print(processed_data.keys())\n",
    "    #'boiler', 'solar thermal pumping station', 'laptop computer', 'washer dryer', 'dish washer'])\n",
    "    #('laptop computer', 'solar thermal pumping station', 'boiler', 'dish washer', 'washer dryer')\n",
    "    AllTable = [[k,v] for k,v in processed_data.items()]\n",
    "    #print(AllTable[0][0])\n",
    "    devicesX_Y = []\n",
    "    dataX_Y = []\n",
    "        \n",
    "\n",
    "            \n",
    "    # for loop goes over all datasets\n",
    "    for i in range(0,number_of_datasets):    \n",
    "        devicesY = []\n",
    "        dataY = []\n",
    "        j = 0\n",
    "        \n",
    "        # while loop goes over all devices in datasets\n",
    "        while j < number_of_devices_in_datasets:\n",
    "            \n",
    "            # we get a random number with random library\n",
    "            random_number = random.randrange(number_of_all_devices)\n",
    "            \n",
    "            # we use the random number to get a random device and random data that belongs to it from AllTable\n",
    "            random_device = AllTable[random_number][0]\n",
    "            random_data = AllTable[random_number][1]\n",
    "            \n",
    "            # we append the device and its data if it doesn't already exist \n",
    "            # and thus avoid having the same device more then once in the same dataset\n",
    "            if random_device not in devicesY:\n",
    "                devicesY.append(random_device)\n",
    "                dataY.append(random_data)\n",
    "                j += 1\n",
    "        \n",
    "        devicesX_Y.append(devicesY)\n",
    "        dataX_Y.append(dataY)\n",
    "    \n",
    "    return devicesX_Y, dataX_Y\n",
    "\n",
    "def ListsToDictlist(devices: list, data: list, number_of_devices: int):\n",
    "    \n",
    "    # This function takes lists from DevicesDataXY or DevicesDataHoly5 and\n",
    "    # turns each of the sublists into a dictionary and then appends those dicitonaries into a list. \n",
    "    \n",
    "    Dictlist=[]\n",
    "    \n",
    "    # we make dictionaries out of lists that we input and append them to list of dictionaries (Dictlist)\n",
    "    for i in range(0,number_of_devices):\n",
    "        dictionary=dict(zip(devices[i], data[i]))\n",
    "        Dictlist.append(dictionary)\n",
    "        \n",
    "    return Dictlist\n",
    "def class_weights_tool(y_test):\n",
    "    \n",
    "    # This function returns class weights for each device in a prticular dataset in a form of a dicitionary.\n",
    "    # It does so by simply counting how many times the device is present througout the dataset.\n",
    "    \n",
    "    # inspired by ronnie coleman\n",
    "    light_weight, nums = [], []\n",
    "    \n",
    "    # this for loop goes over the collumns of the y_test dataset\n",
    "    for j in range(0,len(y_test[0])):             \n",
    "        \n",
    "        count=0\n",
    "        \n",
    "        # gives 0,1,2,3,4,5,6....\n",
    "        nums.append(j)\n",
    "        \n",
    "        # this loop goes over the rows in the y_test dataset\n",
    "        for i in range(0,len(y_test)):\n",
    "            \n",
    "            # we count Trues in the whole column of y_test dataset\n",
    "            if y_test[i][j] == True: count+=1     \n",
    "        \n",
    "        # we append Trues for the column of y_test dataset to the list light_weight\n",
    "        light_weight.append(count)\n",
    "        \n",
    "    # makes the dictionary    \n",
    "    class_weights_dictionary = dict(zip(nums, light_weight))             \n",
    "    \n",
    "    return class_weights_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10c156-25ee-4e3c-ac04-c67a28c07668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Tuple\n",
    "global display_graph_counter\n",
    "display_graph_counter=0\n",
    "\n",
    "\n",
    "\n",
    "def appliance_augmentator(dataset:Dict[str,list], sample_length:int, n_appliances_per_sample:int, random_state:int=None) -> Iterator[Tuple[np.ndarray, np.ndarray, List[str]]]:\n",
    "    global display_graph_counter\n",
    "   \n",
    "    # Initialize seeded random number generator\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    \n",
    "    # How many appliances are mixed together\n",
    "    N = n_appliances_per_sample\n",
    "    \n",
    "    # Sample length / Window size\n",
    "    L = sample_length\n",
    "    \n",
    "    # Noise floor\n",
    "    NOISE_FLOOR = 20.0\n",
    "    \n",
    "    # Get all available appliances\n",
    "    appliance_names = tuple(dataset.keys())\n",
    "    \n",
    "    # How many appliances are there?\n",
    "    n_appliances = len(appliance_names)\n",
    "\n",
    "    # Start endless generator of samples\n",
    "    # pre-allocate array for time series\n",
    "    series = np.zeros(L, dtype=np.float64)\n",
    "        \n",
    "    # pre-allocate boolean array for masks\n",
    "    labels = np.zeros((n_appliances, L), dtype=bool)\n",
    "\n",
    "    if testing:\n",
    "        print(\"test run\")\n",
    "        appliance_names=naprave\n",
    "\n",
    "    print(appliance_names, \" pri augmentarorju\")\n",
    "    \n",
    "\n",
    "    # Select N random appliances (no replace, because same appliance should not appear twice in the same sample)\n",
    "    # TODO: Relax this limitation in the future, for cases where multiple of same appliances are in the same household\n",
    "    while True:\n",
    "        for _ in tqdm(range(120000)):\n",
    "            #print(_)\n",
    "            series = np.zeros(L, dtype=np.float64)\n",
    "\n",
    "            # pre-allocate boolean array for masks\n",
    "            labels = np.zeros((n_appliances, L), dtype=bool)\n",
    "\n",
    "            #print(\"///////////////////////////\")\n",
    "            for appliance_idx in rng.choice(n_appliances, size=N, replace=False):\n",
    "                #print(appliance_names[appliance_idx])\n",
    "                appliance_name = appliance_names[appliance_idx]\n",
    "                \n",
    "                # Pick random sample of a selected appliance\n",
    "                n_available_samples = len(dataset[appliance_name])\n",
    "                empty=True\n",
    "                while empty:\n",
    "                    n_available_samples = len(dataset[appliance_name])\n",
    "                    sample_idx = rng.choice(n_available_samples)\n",
    "                    #print(\"test1\")\n",
    "                    # retrieve sample as NumPy array with appropriate dimensions\n",
    " \n",
    "                    sample_series = dataset[appliance_name][sample_idx].iloc[:].to_numpy().squeeze(axis=-1)\n",
    "                        #with numpy.printoptions(threshold=numpy.inf):\n",
    "                            #print(sample_series)            \n",
    "                        # If sample is too short (shorter than L), give padding on both sides.\n",
    "                    if len(sample_series) <= L:\n",
    "                        padding = L // 2\n",
    "                        sample_series = np.pad(sample_series, (padding, padding), mode='constant', constant_values=0)\n",
    "                        #print(\"////////////////////////\")\n",
    "                        #with numpy.printoptions(threshold=numpy.inf):\n",
    "                            #print(sample_series)\n",
    "                        # The total length of sample time-series\n",
    "                    sample_len = len(sample_series)\n",
    "\n",
    "                    assert sample_len >= L, f'Sample length should be equal or larger than L: {sample_len} >= {L}'\n",
    "\n",
    "                    sample_offset = rng.choice(sample_len - L)\n",
    "\n",
    "                    sample = sample_series[sample_offset:sample_offset+L]\n",
    "\n",
    "                    mask = sample > NOISE_FLOOR  # find samples that are above noise floor\n",
    "                    if (np.any(mask)):                   \n",
    "                        series[:] += sample\n",
    "                        labels[appliance_idx, :] |= mask\n",
    "                        empty=False\n",
    "\n",
    "                    # logical ORing the mask\n",
    "\n",
    "\n",
    "            display_graph_counter+=1\n",
    "            yield series, labels, appliance_names\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0139a0b-87ee-4bfa-887b-6e5f8b74e266",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "093788ea-f785-431e-934b-4b1c6a8cfe66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Dict to give us a list of devices to train with\n",
    "with open('HES_processed', 'rb') as handle:\n",
    "    needed_devices = pickle.load(handle)\n",
    "# Dict with which we will train the model    \n",
    "with open('5merged.pickle', 'rb') as handle:\n",
    "    processed_data = pickle.load(handle)\n",
    "\n",
    "# create a dict with only the required (shared) devices\n",
    "processed_data = {k: v for k, v in processed_data.items() if k in needed_devices}\n",
    "print(len(processed_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a95a6f31-5b82-416c-acb5-3169ce8aefb2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "number of AD/DiT:  2  /  5\n",
      "repetition number:  3 / 3\n",
      "('fridge', 'washing machine', 'air conditioner', 'microwave', 'electric oven')  pri augmentarorju\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 119999/120000 [00:32<00:00, 3720.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU enabled\n",
      "Model: \"LOL2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2550, 1)]         0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv1D)       (None, 2550, 64)          256       \n",
      "                                                                 \n",
      " block1_conv2 (Conv1D)       (None, 2550, 64)          12352     \n",
      "                                                                 \n",
      " block1_pool (AveragePooling  (None, 1275, 64)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " block2_conv1 (Conv1D)       (None, 1275, 128)         24704     \n",
      "                                                                 \n",
      " block2_conv2 (Conv1D)       (None, 1275, 128)         49280     \n",
      "                                                                 \n",
      " block2_pool (AveragePooling  (None, 637, 128)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " block3_conv1 (Conv1D)       (None, 637, 256)          98560     \n",
      "                                                                 \n",
      " block3_conv2 (Conv1D)       (None, 637, 256)          196864    \n",
      "                                                                 \n",
      " block3_pool (AveragePooling  (None, 318, 256)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " block4_conv1 (Conv1D)       (None, 318, 512)          393728    \n",
      "                                                                 \n",
      " block4_conv2 (Conv1D)       (None, 318, 512)          786944    \n",
      "                                                                 \n",
      " block4_pool (AveragePooling  (None, 159, 512)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " block5_tran_conv1 (Conv1DTr  (None, 159, 512)         786944    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " block5_pool (AveragePooling  (None, 79, 512)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                110976    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              266240    \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 5)                 20485     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,528,645\n",
      "Trainable params: 19,528,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "750/750 [==============================] - 25s 32ms/step - loss: inf - accuracy: 0.3665\n",
      "Epoch 2/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.4183\n",
      "Epoch 3/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.4584\n",
      "Epoch 4/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.4809\n",
      "Epoch 5/25\n",
      "750/750 [==============================] - 24s 32ms/step - loss: inf - accuracy: 0.4988\n",
      "Epoch 6/25\n",
      "750/750 [==============================] - 24s 32ms/step - loss: inf - accuracy: 0.5132\n",
      "Epoch 7/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5170\n",
      "Epoch 8/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5278\n",
      "Epoch 9/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5375\n",
      "Epoch 10/25\n",
      "750/750 [==============================] - 23s 31ms/step - loss: inf - accuracy: 0.5449\n",
      "Epoch 11/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5623\n",
      "Epoch 12/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5667\n",
      "Epoch 13/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5689\n",
      "Epoch 14/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5814\n",
      "Epoch 15/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5987\n",
      "Epoch 16/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.6063\n",
      "Epoch 17/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.6149\n",
      "Epoch 18/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.6247\n",
      "Epoch 19/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.6353\n",
      "Epoch 20/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.6392\n",
      "Epoch 21/25\n",
      "750/750 [==============================] - 24s 32ms/step - loss: inf - accuracy: 0.6515\n",
      "Epoch 22/25\n",
      "750/750 [==============================] - 23s 31ms/step - loss: inf - accuracy: 0.6626\n",
      "Epoch 23/25\n",
      "750/750 [==============================] - 23s 31ms/step - loss: inf - accuracy: 0.6747\n",
      "Epoch 24/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.6880\n",
      "Epoch 25/25\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.6935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 11). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./results_5merged_HES_oven1/modeli_fixed/(3)_25ep_bs128_W2550_Devices5_opt2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./results_5merged_HES_oven1/modeli_fixed/(3)_25ep_bs128_W2550_Devices5_opt2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 3s 4ms/step\n",
      "||||||||||||||||||||||||||||||||||||| 2 |||||||||||||||||||||||||||||||||||||\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "         fridge       0.95      0.89      0.92      9533\n",
      "washing machine       0.84      0.94      0.89      9667\n",
      "air conditioner       0.99      0.98      0.99      9651\n",
      "      microwave       0.97      0.83      0.89      9641\n",
      "  electric oven       0.99      0.97      0.98      9508\n",
      "\n",
      "      micro avg       0.94      0.92      0.93     48000\n",
      "      macro avg       0.95      0.92      0.93     48000\n",
      "   weighted avg       0.95      0.92      0.93     48000\n",
      "    samples avg       0.95      0.92      0.93     48000\n",
      "\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "number of AD/DiT:  3  /  5\n",
      "repetition number:  3 / 3\n",
      "('electric oven', 'fridge', 'washing machine', 'microwave', 'air conditioner')  pri augmentarorju\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 119999/120000 [00:43<00:00, 2740.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU enabled\n",
      "Model: \"LOL2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2550, 1)]         0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv1D)       (None, 2550, 64)          256       \n",
      "                                                                 \n",
      " block1_conv2 (Conv1D)       (None, 2550, 64)          12352     \n",
      "                                                                 \n",
      " block1_pool (AveragePooling  (None, 1275, 64)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " block2_conv1 (Conv1D)       (None, 1275, 128)         24704     \n",
      "                                                                 \n",
      " block2_conv2 (Conv1D)       (None, 1275, 128)         49280     \n",
      "                                                                 \n",
      " block2_pool (AveragePooling  (None, 637, 128)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " block3_conv1 (Conv1D)       (None, 637, 256)          98560     \n",
      "                                                                 \n",
      " block3_conv2 (Conv1D)       (None, 637, 256)          196864    \n",
      "                                                                 \n",
      " block3_pool (AveragePooling  (None, 318, 256)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " block4_conv1 (Conv1D)       (None, 318, 512)          393728    \n",
      "                                                                 \n",
      " block4_conv2 (Conv1D)       (None, 318, 512)          786944    \n",
      "                                                                 \n",
      " block4_pool (AveragePooling  (None, 159, 512)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " block5_tran_conv1 (Conv1DTr  (None, 159, 512)         786944    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " block5_pool (AveragePooling  (None, 79, 512)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 64)                110976    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              266240    \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 5)                 20485     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,528,645\n",
      "Trainable params: 19,528,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "750/750 [==============================] - 25s 31ms/step - loss: inf - accuracy: 0.2686\n",
      "Epoch 2/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.4282\n",
      "Epoch 3/20\n",
      "750/750 [==============================] - 23s 31ms/step - loss: inf - accuracy: 0.4459\n",
      "Epoch 4/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.4439\n",
      "Epoch 5/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.4587\n",
      "Epoch 6/20\n",
      "750/750 [==============================] - 23s 31ms/step - loss: inf - accuracy: 0.4884\n",
      "Epoch 7/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5069\n",
      "Epoch 8/20\n",
      "750/750 [==============================] - 23s 31ms/step - loss: inf - accuracy: 0.5167\n",
      "Epoch 9/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5377\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5649\n",
      "Epoch 11/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5779\n",
      "Epoch 12/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.5969\n",
      "Epoch 13/20\n",
      "750/750 [==============================] - 24s 32ms/step - loss: inf - accuracy: 0.6210\n",
      "Epoch 14/20\n",
      "750/750 [==============================] - 24s 32ms/step - loss: inf - accuracy: 0.6309\n",
      "Epoch 15/20\n",
      "750/750 [==============================] - 23s 31ms/step - loss: inf - accuracy: 0.6357\n",
      "Epoch 16/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.6539\n",
      "Epoch 17/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.6697\n",
      "Epoch 18/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.6819\n",
      "Epoch 19/20\n",
      "750/750 [==============================] - 24s 31ms/step - loss: inf - accuracy: 0.6954\n",
      "Epoch 20/20\n",
      "750/750 [==============================] - 23s 31ms/step - loss: inf - accuracy: 0.7093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 11). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./results_5merged_HES_oven1/modeli_fixed/(3)_20ep_bs128_W2550_Devices5_opt3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./results_5merged_HES_oven1/modeli_fixed/(3)_20ep_bs128_W2550_Devices5_opt3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 3s 4ms/step\n",
      "||||||||||||||||||||||||||||||||||||| 2 |||||||||||||||||||||||||||||||||||||\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  electric oven       0.97      0.97      0.97     14358\n",
      "         fridge       0.91      0.89      0.90     14391\n",
      "washing machine       0.92      0.91      0.91     14408\n",
      "      microwave       0.92      0.85      0.88     14408\n",
      "air conditioner       0.99      0.98      0.98     14435\n",
      "\n",
      "      micro avg       0.94      0.92      0.93     72000\n",
      "      macro avg       0.94      0.92      0.93     72000\n",
      "   weighted avg       0.94      0.92      0.93     72000\n",
      "    samples avg       0.95      0.92      0.93     72000\n",
      "\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import gc\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "date = \"28.9.2022\"\n",
    "net_name = 'LOL2'\n",
    "window_size= 2550\n",
    "batch_size = 512\n",
    "NmSets = 3\n",
    "learning_rate = 0.001\n",
    "testing=False\n",
    "\n",
    "#devicesX_Y, dataX_Y = DevicesDataXY(NmSets,NmDevices,54)\n",
    "\n",
    "logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "for tačka in range(0,2):\n",
    "    mačka = [2,3]\n",
    "    options = mačka[tačka]\n",
    "    for turtle in range(0,1):\n",
    "        beno = [5]\n",
    "        NmDevices = beno[turtle]\n",
    "        devicesX_Y, dataX_Y = DevicesDataXY(NmSets,NmDevices,5) # last parameter is 22 for refit and 54 for ukdale\n",
    "        \n",
    "        Dictlist = ListsToDictlist(devicesX_Y, dataX_Y, NmSets)\n",
    "        if options < NmDevices: \n",
    "            for nm in range(2,NmSets):\n",
    "                # |||||||||||||||||||||||||||||||| parameters ||||||||||||||||||||||||||||||||\n",
    "                print(\"number of AD/DiT: \", options, \" / \", NmDevices)\n",
    "                print(\"repetition number: \", nm+1, \"/\", NmSets)\n",
    "                if options == 1: \n",
    "                    epochs = 30\n",
    "                    learning_rate = 0.0001\n",
    "                elif options == 2: \n",
    "                    epochs = 25\n",
    "                    learning_rate = 0.0001                              \n",
    "                else: \n",
    "                    epochs = 20    \n",
    "                    learning_rate = 0.0003\n",
    "                    #learning_rate = 0.0003 # dej 0.0003 kot default za LOL2 (ecopirnat) (0,0001 za vgg)\n",
    "                batch_size=128\n",
    "                # |||||||||||||||||||||||||||||||| dataengineering ||||||||||||||||||||||||||||||||\n",
    "                x_train, x_test, y_train, y_test, labels = Generate4Upgraded(Dictlist, window_size, options, nm, 120000)\n",
    "\n",
    "                pickle.dump(labels, open(f'./results_5merged_HES_oven1/naprave/devices_({nm+1})_{epochs}ep_bs{batch_size}_W{window_size}_Devices{NmDevices}_opt{options}.pkl', 'wb'))\n",
    "                class_weights = class_weights_tool(y_test)\n",
    "                # |||||||||||||||||||||||||||||||| DL ||||||||||||||||||||||||||||||||\n",
    "                model = LOL2(NmDevices, window_size, 'gru', 64)\n",
    "                #model = VGG11_1D(NmDevices, window_size)\n",
    "                #model = Tanoni()\n",
    "                model.build((len(y_train)+len(y_test),window_size,1))\n",
    "                model.summary()\n",
    "                \n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "                model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, class_weight=class_weights)\n",
    "                \n",
    "\n",
    "                model.save(f'./results_5merged_HES_oven1/modeli_fixed/({nm+1})_{epochs}ep_bs{batch_size}_W{window_size}_Devices{NmDevices}_opt{options}/')\n",
    "                #model.save(f'./models/rawTS/LOL2/(1)_20ep_bs128_W2550_Devices10_opt9/')\n",
    "                SaveData = [x_train, x_test, y_train, y_test, labels]\n",
    "                #pickle.dump(SaveData, open(f'./results_5merged_HES/dataseti/dataset_({nm+1})_{epochs}ep_bs{batch_size}_W{window_size}_Devices{NmDevices}_opt{options}.pkl', 'wb'))\n",
    "                # |||||||||||||||||||||||||||||||| prediction boiler||||||||||||||||||||||||||||||||\n",
    "                y_pred = model.predict(x_test)\n",
    "                y_pred_tf = (y_pred > 0.5)\n",
    "                print(f\"||||||||||||||||||||||||||||||||||||| {nm} |||||||||||||||||||||||||||||||||||||\")\n",
    "                print(metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0))\n",
    "                F1weighted = metrics.f1_score(y_test, y_pred_tf, labels=None, pos_label=1, average='weighted', sample_weight=None, zero_division=0)\n",
    "                pickle.dump(F1weighted, open(f'./results_5merged_HES/rezultati/({nm+1})_F1Weighted_NmDevices{NmDevices}_opt{options}','wb'))\n",
    "                print(\"|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0a21c-69a2-4aef-829f-d238d99f5e13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "242dcc7b-0d7b-4fcb-8642-90bec094e4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "---------------------------------------------------------------------------\n",
      "number of AD/DiT:  4  /  5\n",
      "repetition number:  2 / 2\n",
      "('microwave', 'washing machine', 'air conditioner', 'fridge', 'electric oven') original dataset\n",
      "po spremembi dict_keys(['microwave', 'fridge', 'washing machine', 'electric oven', 'air conditioner'])\n",
      "test run\n",
      "('microwave', 'washing machine', 'air conditioner', 'fridge', 'electric oven')  pri augmentarorju\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 119999/120000 [00:50<00:00, 2392.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True False]\n",
      " [False  True  True  True  True]\n",
      " [ True  True  True False  True]\n",
      " ...\n",
      " [ True False  True  True  True]\n",
      " [ True  True  True  True False]\n",
      " [ True  True False  True  True]]\n",
      "750/750 [==============================] - 3s 4ms/step\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      microwave       0.81      0.97      0.88     19142\n",
      "washing machine       0.82      0.88      0.85     19235\n",
      "air conditioner       0.92      0.94      0.93     19273\n",
      "         fridge       0.82      0.93      0.87     19089\n",
      "  electric oven       0.81      0.90      0.86     19261\n",
      "\n",
      "      micro avg       0.83      0.92      0.88     96000\n",
      "      macro avg       0.84      0.92      0.88     96000\n",
      "   weighted avg       0.84      0.92      0.88     96000\n",
      "    samples avg       0.84      0.92      0.88     96000\n",
      "\n",
      "Accuracy with  5 devices and 4 options is:  0.8768019793984421\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[254], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy with  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNmDevices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m devices and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m options is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, f1score)\n\u001b[1;32m     91\u001b[0m         results[turtle][tačka]\u001b[38;5;241m.\u001b[39mappend(f1score)\n\u001b[0;32m---> 92\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy with  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNmDevices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m devices and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m options is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mturtle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtačka\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnm\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m2\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     95\u001b[0m     results[turtle][tačka]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import keras\n",
    "def avg(lst):\n",
    "    return sum(lst)/len(lst)\n",
    "global display_graph_counter\n",
    "display_graph_counter=0\n",
    "iteration = 13\n",
    "date = \"28.9.2022\"\n",
    "epochs = 20\n",
    "net_name = 'VGG11_1D'\n",
    "window_size= 2550\n",
    "batch_size = 128\n",
    "NmSets = 2\n",
    "NmDevices = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "testing=True\n",
    "    \n",
    "\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "for tačka in range(0,6):\n",
    "    mačka = [4]\n",
    "    options = mačka[tačka]\n",
    "    for turtle in range(0,3):\n",
    "        beno = [5]\n",
    "        NmDevices = beno[turtle]\n",
    "        #NmDevices = beno[turtle]\n",
    "        #devicesX_Y, dataX_Y = DevicesDataXY(NmSets,NmDevices,54) # last parameter is 22 for refit and 54 for ukdale\n",
    "        #Dictlist = ListsToDictlist(devicesX_Y, dataX_Y, NmSets)\n",
    "        if options < NmDevices: \n",
    "            for nm in range(1,NmSets):\n",
    "                # |||||||||||||||||||||||||||||||| parameters ||||||||||||||||||||||||||||||||\n",
    "                \n",
    "                print(\"number of AD/DiT: \", options, \" / \", NmDevices)\n",
    "                print(\"repetition number: \", nm+1, \"/\", NmSets)\n",
    "                \n",
    "                data = pickle.load(open('results_5merged_HES_oven1/naprave/devices_(3)_20ep_bs512_W2550_Devices5_opt4.pkl', 'rb'))\n",
    "                naprave=data\n",
    "                print(naprave, \"original dataset\")\n",
    "                \n",
    "                \n",
    "                processed_data_copy=processed_data.copy()\n",
    "                for k, v in processed_data_copy.items():\n",
    "                    if k not in naprave:\n",
    "                        del processed_data[k]\n",
    "                        \n",
    "                print(\"po spremembi\", processed_data.keys())\n",
    "                \n",
    "                devicesX_Y, dataX_Y = DevicesDataXY(NmSets,NmDevices,5) # last parameter is 22 for refit and 54 for ukdale\n",
    "                Dictlist = ListsToDictlist(devicesX_Y, dataX_Y, NmSets)\n",
    "                x_train, x_test, y_train, y_test, labels = Generate4Upgraded(Dictlist, window_size, options, nm, 120000)\n",
    "                data = [x_train, x_test, y_train, y_test, labels]\n",
    "                class_weights = class_weights_tool(y_test) \n",
    " \n",
    "                saved_model = keras.models.load_model('results_5merged_HES_oven1/modeli_fixed/(3)_20ep_bs128_W2550_Devices5_opt4')\n",
    "                y_pred = saved_model.predict(data[1])\n",
    "        \n",
    "                y_pred_tf = (y_pred > 0.5)\n",
    "                #print(y_pred_tf)\n",
    "                report = metrics.classification_report(data[3], y_pred_tf, target_names=data[4], zero_division=0)\n",
    "                f1score = metrics.f1_score(data[3], y_pred_tf, labels=None, pos_label=1, average='weighted', sample_weight=None, zero_division=0)\n",
    "                \n",
    "                print(metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0))\n",
    "\n",
    "                print(f\"Accuracy with  {NmDevices} devices and {options} options is: \", f1score)\n",
    "\n",
    "            \n",
    "        else: \n",
    "            results[turtle][tačka].append(0)\n",
    "        FinalResults[turtle].append(round(avg(results[turtle][tačka])*100,2))\n",
    "        \n",
    "print(FinalResults)\n",
    "pickle.dump(FinalResults, open(f'./datasets/rawTS/| LOL2_DALE |/FinalResults_2.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
